ダンスは、音楽のリズム、音色、音色の特徴に従った複雑な動きを巧みに構成することが必要である。音楽を条件としたダンスの生成は、形式的には、音声信号を条件とした高次元連続運動信号のモデル化の問題として表現することができる。本研究では、この問題に取り組むために、2つの貢献を行う。まず、マルチモーダル変換エンコーダを用いて、音楽文脈だけでなく以前のポーズを条件とする正規化フローで将来のポーズに関する分布をモデル化する、新しい確率的自己回帰アーキテクチャを提示する。次に、様々なモーションキャプチャ技術によって得られた、プロとカジュアルダンサーの両方を含む、現在最大の3Dダンスモーションデータセットを紹介する。このデータセットを用いて、我々の新しいモデルを客観的な指標とユーザ調査によって2つのベースラインと比較し、確率分布をモデル化する能力と、大きな動きと音楽のコンテキストに参加する能力の両方が、音楽にマッチした面白く、多様で、リアルなダンスを作り出すために必要であることを示している。

1 はじめに ダンス（音楽とともに行う体の動き）は、文化の壁を越えた人間の深い営みであり、私たちは「踊る種」（LaMothe 2019）と呼ばれています。今日、YouTubeやTikTokなどのデジタル動画プラットフォームでは、ダンスを含むコンテンツが最も多く視聴されています。近年のパンデミックによって、ダンスは--他の舞台芸術と同様に--ますますバーチャルな実践となり、それゆえますますデジタル化された文化表現となった。しかし、アナログであれデジタルであれ、優れたダンスを創り出すのは難しいことです。プロのダンスには、身体能力と幅広い練習が必要です。デジタル手段で同様の体験をキャプチャしたり再現したりするには、モーションキャプチャやハンドアニメーションなど、多大な労力を必要とします。その結果，近年，データ駆動型の自動ダンス生成の問題に関心が集まっている [Li et al.］ ダンスの生成モデルにアクセスすることは，クリエイターやアニメーターにとって，ワークフローの高速化，インスピレーションの提供，ユーザの選曲にリアルタイムで反応するインタラクティブなキャラクターの作成など，新しい可能性を開くことにつながるだろう．また，同じモデルによって，人間が音楽と動きをどのように結びつけているかを知ることができる．この2つは，人間の認知の重要かつ相互に関連した側面を捉えていると特定されている［Bläsing et al.2012］．

ダンスが体現する運動学的プロセスは、ロコモーションのような他の人間の運動と比較しても、非常に複雑で非線形である。さらにダンスはマルチモーダルであり、音楽とダンスモーションの結びつきは極めて多面的であり、決定論的とは言い難い。ディープニューラルネットワークを用いた生成モデリングは、このような複雑な領域の表現を学習するための最も有望なアプローチの1つになりつつあります。この一般的なアプローチは、画像 [Brock et al. 2018; Karras et al. 2019; Park et al. 2019], 音楽 [Dhariwal et al. 2020; Huang et al. 2018], 運動 [Henter et al. 2020; Ling et al. 2020], 音声 [Prenger et al. 2019; Shen et al. 2018] そして自然言語 [Brown et al. 2019] のドメインにおいて既に大きな進展があった。最近では、言語と画像の間[Ramesh et al. 2021]、言語とビデオの間[Wu et al. 2021]など、標準データ領域間のさらに複雑な相互作用を捉えることを学習する、マルチモーダルモデルが開発されつつある。同様に、ダンス合成は動作モデリングと音楽理解の交差点に位置し、説得力のある機械学習の課題と明確な社会文化的インパクトを併せ持つ刺激的な問題である。この研究では、深層学習を通じて、音楽条件付き3Dダンスモーション生成の問題に取り組む。特に、この困難な課題に対するモデルのパフォーマンスに影響を与える2つの重要な要素、1）長時間にわたって拡張されたパターンを捉える能力、2）予測された出力に対する複雑な確率分布を表現する能力について探求している。我々は、これまでの作品はこの2つの特性のいずれかが欠けていると主張し、マルチモーダル文脈（前の動き、前と未来の両方の音楽）を符号化するトランスフォーマー[Vaswani et al. 2017]と、ダンス合成のための未来の動きである予測モダリティ上の未来の分布を忠実にモデル化する正規化フロー[Papamakarios et al. 2021] ヘッドを結合した新しい自己回帰神経アーキテクチャを提示する。我々はこの新しいアーキテクチャをTransflowerと呼び、客観的な指標と人間の評価研究を通じて、これらの要素の両方が、ダンスの複雑な動きの分布と音楽のモダリティへの依存をモデル化するために重要であることを示す。人間による評価は、生成モデルの知覚的品質を評価するためのゴールドスタンダードであり、客観的な指標を補完するものである。さらに、YouTubeからダウンロードした任意の "in-the-wild "曲でモデルを評価することが可能であり、そのような曲ではグランドトゥルースのダンスモーションが利用できない。学習ベースの動作合成における最大の課題の1つは、3D動作の大規模なデータセットが利用可能かどうかということである。既存のデータセットは主に2つの方法で収集されています：モーションキャプチャースタジオ[CMU Graphics Lab 2003; Ferstl and McDonnell 2018; Lee et al. 2019a; Mahmood et al. 2015; Troje 2002]は、最高品質のモーションを提供するが、高価な装置を必要とし、より大きなデータセットサイズにスケールすることが困難である、またはビデオからの単眼3D姿勢推定を介して[Habibieら2021; Pengら2018b]、インターネットからのビデオのはるかに大きな可用性のために品質をトレードオフするものである。本論文では、異なるソースとモーションキャプチャ技術を組み合わせた、3Dダンスモーションの最大のデータセットを紹介します。我々は、大規模なモーションデータセットを取得するための新しいアプローチを導入し、以前の作品で主に使用されている2つのアプローチを補完する。具体的には、バーチャルリアリティ（VR）技術、特にVRダンスの人気とユーザー数の増加 [Lang 2021] を利用し、我々の研究にダンスデータを提供することに興味を持つ参加者を探し出す。我々は、消費者グレードのVRモーションキャプチャは、プロのモーションキャプチャほど高品質ではないが、ビデオからの現在の単眼3Dポーズ推定よりも大幅に優れており、より堅牢であることを主張する。さらに、VR市場の成長に伴い、品質と可用性の両方が向上し[Statista 2020]、参加型研究のための新しい道を提供する可能性があるとしている。

また、我々は、カジュアルなダンサーとプロのダンサーの両方から、プロのモーションキャプチャ装置を使用してダンスモーションの最大のデータセットを収集します。最後に、我々のモデルを学習するために、我々の新しいデータを、2つの既存の3Dダンスモーションデータセット、GrooveNet [Alemi et al. 2017] とAIST++ [Li et al. 2021a] と組み合わせ、我々は共通のスケルトンに標準化します。合計20時間以上のダンスデータは、フリースタイル、カジュアルダンス、ストリートダンススタイルなど様々なダンススタイルと、ポップ、ヒップホップ、トラップ、K-POP、ストリートダンスミュージックなど様々な音楽ジャンルのものがあります。さらに、すべてのデータソースは、単眼映像から推定した3Dモーションデータよりも高品質なモーションキャプチャを提供しますが、異なるソースは、異なる品質レベル、異なるキャプチャアーチファクトを提供します。このようなデータソースの多様性に加え、ダンススタイルやスキルレベルの大きな多様性により、決定論的モデルはデータを忠実にモデル化するために収束できないが、確率的モデルはこのような異種素材に適応できることを見出した。我々の貢献は以下の通りである。- 高次元連続信号の自己回帰確率的モデリングのための新しいアーキテクチャを提示し、音楽条件付きダンス生成のタスクで最先端の性能を達成することを実証する。このアーキテクチャは、我々の知る限り、シーケンスモデリングのためのトランスフォーマーと、確率的モデリングのための正規化フローを組み合わせた最初のものである。- 我々は、様々なモーションキャプチャシステムで生成された3Dダンスモーションの最大のデータセットを紹介する。このデータセットは、参加型研究およびモーキャプの民主化のためのVRの可能性を示すものでもある。- 我々は、2つのベースラインに対して、我々の新しいモデルを客観的かつユーザースタディで評価し、音楽にマッチした自然で多様なダンスを生成するためには、確率的注意とマルチモーダル注意の両方のコンポーネントが重要であることを示す。- 最後に、ダンスの品質とスタイルの制御を達成するために、微調整と「モーション・プロンプト」の使用を検討する。本論文のウェブサイト（metagen.ai／transflower）では、データ、コード、学習済みモデル、ビデオ、補足資料、および、任意の曲で開始モーションを選択しモデルをテストするデモを提供しています。

2 背景と先行研究 2.1 学習ベースのモーション合成 3Dモーション生成のタスクは、様々な方法で取り組まれてきた。従来の動作合成のアプローチは、動作データベースや動作グラフからの検索に基づいていた[Arikan and Forsyth 2002; Chao et al. 2004; Kovar and Gleicher 2004; Kovar et al.］ 近年では、統計的手法や学習ベースの手法が注目されており、より大規模なデータセットやより複雑なタスクに対して柔軟に拡張することができる。Holdenら[2020]は、従来の検索ベースの動作合成アプローチと、よりスケーラブルな深層学習ベースのアプローチの連続性を探求し、両者のアイデアを組み合わせることが有益であることを示している。学習ベースの手法のうち、ほとんどの作品は自己回帰的アプローチに従っており、シーケンス内の次のポーズまたは次のキーポーズのいずれかが、シーケンス内の以前のポーズに基づいて予測されます。ダンスでは、予測は音楽の特徴も条件とし、典型的には予測時刻の周りの時間の窓をまたぐ。ここでは、前のポーズとこの音楽特徴のウィンドウを合わせてコンテキストと呼ぶ。自己回帰手法は、冒頭で提案した要因、すなわち、自己回帰モデルが入力（コンテキスト）を処理する方法と、出力（予測される動き）を処理する方法に沿って分類することができる。また、本節の後半では、仮定の量や学習アルゴリズムの種類によってアプローチを比較する。文脈依存性。我々は、より広いコンテクストウィンドウからの情報をより効果的に保持し利用するモデルへの進化を目にしてきた。ニューラルネットワークを運動予測に適用した最初の作品は、LSTMのようなリカレントニューラルネットワークに依存し、無条件運動［Fragkiadaki et al. 2015; Zhou et al. 2018］とダンス合成［Crnkovic-Friis and CrnkovicFriis 2016; Tang et al. 2018］に適用された。LSTM は最近の文脈を潜在的な状態で表現する。しかし、この潜在的な状態は、過去のコンテキストからの情報の流れを妨げるボトルネックとして機能することができ、したがって、ネットワークが学習できる時間的相関の範囲を制限することができる。この問題に対処するために、さまざまなアーキテクチャが用いられてきた。Bütepageら[2017]、Holdenら[2017]、Starkeら[2020]は、フィードフォワードネットワークを通じて最近のポーズの履歴を直接フィードして次のポーズを予測し、Zhuangら[2020]はWaveNetスタイルのアーキテクチャを用いてダンス合成のためにコンテキストをさらに拡張しています。最近、Liら[2021a]は、ダンス生成のために、最後の2秒間の動きと同様に、隣接する4秒間の音楽に関連する特徴に注目するように学習するクロスモーダル変換器アーキテクチャ（[Vaswaniら2017]を拡張）を導入している。出力モデリング。モーション合成のほとんどの作品は、次のポーズ予測をコンテキストの決定論的関数として扱ってきました。これには、LSTMSを用いた初期の作品［Fragkiadaki et al. 2015; Tang et al. 2018; Zhou et al. 2018］、およびダンス生成に関する最新の作品の一部［Li et al. 2021b,a］ が含まれます。しかし、多くの状況において、出力動作は入力コンテキストによって高度に制約されない。例えば、音楽と一緒に踊れるもっともらしいダンスの動きや、発話によく合う多くの異なるジェスチャーがある[Alexanderson et al.2020]。動作に関する確率的分布をモデル化する以前のアプローチには、ガウス混合モデル及びガウス過程潜在変数モデル［Grochow et al.2004; Levine et al.2012; Wang et al.2008］が含まれる。VAEはガウス性の仮定を弱め、モーション合成に適用されている[Habibie et al.2017; Ling et al.2020]．最近、Petrovichら[2021]は、非自己回帰的なモーション合成のために変換器と組み合わせたVAEを使用しました - 彼らは、全注意変換器の出力として、「一度に」全体のモーションを予測するのです。彼らのアーキテクチャは、複雑なマルチモーダル動作分布の学習を可能にするが、その非自己漸進的アプローチは、生成されたシーケンスの長さを制限する。MoGlow [Henter et al. 2020] は自己回帰正規化フローでモーションをモデル化し、正確な尤度最大化で柔軟な確率分布のフィッティングを可能にし、最先端のモーション合成結果を生成している。Liらでは、関節角度空間を離散化する。しかし、彼らのマルチソフトマックス出力分布は、各関節の独立性を仮定しており、これは多くの場合、異常である。Leeら[2019b]は，運動の潜在的な表現上の分布を予測する音楽条件付きGANを開発している．しかし、学習を安定させるために、彼らは潜在的な表現に対してMSE正則化損失を適用しており、複雑な分布をモデル化する能力を制限している可能性がある。また、Liら[2021b]は敵対的に学習させたモデルを導入しているが、これはノイズ入力（ランダム性の源）を持たないため、音楽条件付き確率分布をモデル化しているとは言い難い1。

ドメイン固有の仮定。モーション合成の異なるアプローチを比較する3つ目の側面は、ドメイン固有の仮定の量です。決定論的モデルからの予測を曖昧にするために、またはモーションのリアリズムを高めるために、異なる作品は、足の接触［Holden et al. 2016］、ペース［Pavllo et al. 2018］、および位相情報［Holden et al. 2017; Starke et al. 2020］などアニメーションの望ましいタイプに合わせた追加の入力をモデルに追加しています。ダンス合成のために、Leeら[2019b]とLiら[2021b]は、ダンスがしばしば実りある短い動きセグメントに分解され、その遷移は音楽の拍にあるという観察を利用しています。さらにLiら[2021b]は、運動学的骨格に特有の帰納的バイアスと、局所的時間相関の学習に対するバイアスを含むアーキテクチャを開発している。一方、Henterら[2020]は、データについてほとんど仮定をしない一般的なシーケンス予測モデルを提示している。これにより、モデルを根本的に変更することなく、ヒューマノイドや四足歩行の動作合成、あるいはジェスチャー生成[Alexanderson et al. ダンス合成については、Liら[2021a]が同様に汎用的なモデルで印象的な結果を得られることを実証している。学習アルゴリズム。モーションキャプチャデータからリアルな動きを生成するために学習する別のアプローチは、模倣学習（IL）として知られる一連の技術を使用することである。Merelら[2017]は、GANと密接に関連する技術である生成的敵対的模倣学習（GAIL）を使用して、物理ベースのヒューマノイドが異なる歩行運動を模倣するためのコントローラを学習しています。Pengら[2018a]とPengら[2021]はこの研究を、多様な形態を持つスキルを学習するキャラクターへと拡張しています。このアプローチは、物理ベースの環境におけるモキャプデータを模倣して学習することができるので、キャラクターの動きは自動的に物理的にリアルになり、自然な動作に必要ですが十分ではありません。また、Lingら[2020]は、ILアプローチと先に述べた教師あり学習、自己教師あり学習のアプローチを組み合わせることで、両者の利点を得ることができる有望な方向であることを示した。全体として、学習ベースのモーション合成は、機械学習が複雑なデータ分布に適用される他の分野と同様の傾向をたどるだろうと予想している：トランスフォーマーのように、大きなコンテキストに柔軟に参加できる一方で、その出力に対して複雑な分布をモデル化できるモデルは、十分なデータが利用できる場合、最高の生成結果を生む［Brown et al.2020; Dosovitskiy et al.2020; Henighan et al.2020; Kaplan et al.2020; Ramesh et al.2021］. ここでは、モーションキャプチャデータセットのサイズと多様性の両方が大きくなるにつれて重要になると思われる、これらの望ましい特性の両方を組み合わせた自己回帰型モーション合成のための最初のモデルであると考えられるものを提示する。さらに、このモデルを音楽条件付きダンス生成に使用し、複数のモダリティ（我々の場合は音楽と動き）を含むタスクに使用できることを実証する。2.2 その他のデータ駆動型ダンス動作合成 この研究の焦点は、ダンス合成に対する純粋な学習ベースのアプローチですが、ダンスを生成するためのデータ駆動型の手法はこれだけではありません。2.1節で述べたように、実際には、完全に非学習ベースのアプローチと純粋に学習ベースのアプローチの間には連続性があります。一般に、学習ベースの手法は、学習のための大きな計算コストと、推論時のコスト削減をトレードオフにしている [Holden et al.2020]が、学習コストを償却することが多い。また、ダンス合成パイプラインに機械学習技術を導入できるいくつかの次元があります。ここでは、モーション合成の部分に焦点を当てる。いくつかの作品は、モーショングラフを使用してダンス生成の問題にアプローチしています。ここでは、事前に作成されたモーションクリップ間の遷移を使用して振り付けを作成します。これらの手法は，事実上，選択によってモーションを生成しているが，深層学習は，より補間に近いと見ることができる．Fanら[2011]とFukayama and Goto[2015]はグラフをトラバースするために統計的手法を使用したが，最近ではモーショングラフをトラバースするために深層学習技術が開発されている[Kang et al.2021; Ye et al.2020]．これらのアプローチでは，信頼性が高く高品質なダンスが生成される傾向にあり，編集が容易である．しかし，モーショングラフに基づくダンス合成の多くは，キーフレーム付きアニメーションのデータセットに依存しており，生成されたダンスが自然でないように見えることがある．そこで、今回紹介する大規模なモーキャップデータベースを利用することで、より自然なモーションを生成できる可能性があります。

2.3 ダンスデータセット ダンス生成に関するこれまでの研究は、様々な異なる手法から得られたデータを用いて行われてきた。Alemiら[2017]とZhuangら[2020]は、音楽に同期したダンスのモキャプデータセットを記録し、それぞれ合計23分と58分である。Liら[2021a]は、産総研ダンスデータセット[Tsuchida et al. 2019]を用いて、マルチビュー3Dポーズ推定を用いた3Dダンスモーションを得ている。Leeら[2019b]とLiら[2020]は、YouTube動画からの単眼3Dポーズ推定を用いて、それぞれ合計71時間、50時間の3Dダンスモーションのデータセットを生成している。単眼3Dポーズ推定は活発な研究分野であるが[Bogo et al. 2016; Mathis et al. 2020; Rong et al. 2021]、現在の方法は不正確なルートモーション推定に悩まされており[Li et al. 2021a] 、これらの作品は関節動作に焦点を当てる傾向にある。最後に、Liら[2021b]は、プロのアニメーターによって作成された5時間のアニメーションダンスデータセットを紹介しています。全体として、データの品質とデータの可用性の間にトレードオフがあることが分かる。本研究では、遠隔地のVRユーザ参加者からモーションデータを収集する新しい方法を提示し、現在利用可能なパレートフロンティアを押し広げ、データ品質はモーキャップ装置のそれに近づき、VRユーザ数が増加するにつれて可用性は増加することを示します。VRを用いたモーションキャプチャーの民主化は、研究者やVRユーザーにとってエキサイティングな可能性をもたらすと思います。特に、大規模なデータをクラウドソースできることで、他の多くの生成モデリングタスクで見られるスケーリング現象 [Henighan et al. 2020] を利用することが可能になり、モデルの性能に対するスケーリングの効果も研究できるようになるはずです。AIST++[Liら2021a]のようなデータセットと比較すると、我々のデータセットはより多様なスキルレベルやAIST++にはないダンススタイルを捉えることができると考えています。本論文では、音楽と同期した3Dダンスモーションの2つの新しい大規模データセット、PMSDダンスデータセット、およびShaderMotion VRダンスデータセットを紹介します。我々は、これらのデータセットを、2つの既存のダンスデータセット、AIST++ [Li et al. 2021a] とGrooveNet [Alemi et al. 2017] と組み合わせて、我々のモデルを訓練するための統合データセットを作成します。我々は、すべてのデータセットを共通のスケルトン（PMSDデータセットに使用されたもの）に標準化し、データを一般に公開する予定である。ここでは、2つの新しいデータセットについて説明し、既存のものを含むすべてのソースを表1で比較します。ポピュラーミュージック＆ストリートダンス(PMSD)データセット。PMSDダンスデータセットは、様々なダンサーやダンススタイルの音声とモーションキャプチャーの同期記録から構成されています。データはOptitrack Prime41モーションキャプチャシステム（120Hzで17台のカメラ）で収録され、2つのパートに分かれています。最初のパート（PMSDCasual）は、7人のノンプロフェッショナルなダンサーがポピュラーな音楽に合わせてカジュアルに踊る様子を142分収録しています。37個のマーカーは、21個の関節を持つ骨格に分解されました。2つ目のパート(PMSDStreet)は、1人のプロのダンサーによる44分のストリートダンスを収録しています。ダンスは3つのスタイルに分かれています。ヒップホップ、ポッピング、クランピングです。音楽はそれぞれのスタイルに適したものがダンサーによって選択されました。この設定では、65個のマーカー（胴体に45個、各指に2個）を使用し、指とヒンジ付きつま先を含む51個の関節を持つスケルトンにデータを解いた。ストリートダンスは、カジュアルダンスに比べ、テンポの変化が多く、繰り返しの少ない、かなり複雑な振り付けである。

ShaderMotion VRダンスデータセット。このデータは、ソーシャルVRプラットフォームであるVRChat2において、アバターの動きを動画にエンコードするShaderMotionというツールを使って、ダンスをする参加者によって記録されました[lox9973 2021]。アバターは、VRヘッドセットとHTC Viveトラッカーを使って、頭、手、腰、足の6点トラッキングシステムで実際の体の動きに固定され、インバースキネマティクスを使って自分の動きを追います。エンコードされた動画は、ShaderMotionリポジトリで提供されているMotionPlayerスクリプトによって、スケルトンリグの動きに変換することができます。データには指のトラッキングも含まれており、その精度は使用するVR機器に依存します。さらに、データを4つのコンポーネントに分割し、それぞれ異なるダンサーとスタイルを持つようにしました（表1参照）。残りのデータは最近入手したものなので、学習用にはShaderMotionの一部（ShaderMotion1とShaderMotionVibeコンポーネント）のみを含んでいます。近々、全データで学習させたモデルを公開する予定です。本データセットのVRダンスがどのように見えるか、ストリートダンススタイルの例として、こちらのURL https:／／www.youtube.com／playlist?list= PLmwqDOin_Zt4WCMWqoK6SdHlg0C_WeCP6 Syrtos dataset.で見ることができる。Syrtosデータセットは、ギリシャの特定のダンススタイルであるクレタ島のSyrtosを、6人のダンサーが録音した音声とモーションキャプチャーを同期させたものです。このデータには11のパフォーマンスが含まれており、1人を除くすべてのダンサーが2回パフォーマンスを行い、合計で約50分の時間が費やされています。データは，2019年にクレタ島で行われた民族誌的フィールドワークの際に，ダンサーと個別に記録され［Holzapfel et al.2020］，240 Hzで動作する17個のセンサーを備えたXsens3慣性システムで，Xsensソフトウェアを通じて22個の関節を持つ骨格に後処理された．音声データは，録音時にミュージシャンが生演奏したもので，個々の楽器を含む単一のトラックで構成されている．なお、今回の研究ではSyrtosのデータは使用せず、今後の研究のために公開しています。

4 方法 我々はトランスフラワーと呼ぶ注意を用いた新しい自己漸進的確率モデルを導入する。このアーキテクチャは、Liら[2021a]のトランスフォーマーを用いた自己漸進的クロスモーダル注意のアイデアと、Henterら[2020]の正規化フローを用いた自己漸進的確率モデルのアイデアを融合したものである。このモデルは、複数のモダリティを入出力とするあらゆる自己回帰的確率的モデリングに適用できるように設計されているが、本論文では音楽条件付き動作生成への適用に焦点を当てる。コードと学習済みモデルは公開される予定である。我々はダンス条件付きモーションを自己漸進的にモデル化する。そのために、動きをポーズのシーケンスx = {𝑥𝑖 }として表現する。𝑖=u↪L_1D441↩ 𝑖=1∈R 𝑥 サンプリングレートは固定で𝑁 回であり、音楽は音声特徴量{𝑚𝑖}のシーケンスとして表現する。𝑖=ᵁ 𝑖=1∈R 𝑑 ᵆ𝑖と同じ時間を中心とした窓から抽出された音声特徴量{ᵅ}の列として音楽を表現する。ᑑ𝑥とᑚは、ポーズと音楽のサンプルの特徴数である。我々の実験では、モーションポーズと音楽の特徴を20Hzでサンプリングする。自己回帰タスクは、以前の全てのポーズといくつかの音楽コンテキストが与えられたときの𝑖番目のポーズを予測することである。簡単のために、予測は以前のᑘポーズと以前のᑘと将来のᑘ音楽特徴にのみ依存するように制限する。ここで、0より小さいインデックスの𝑥または𝑚はパディングされるか、モデルに供給される「コンテキストシード」（初期𝑘ポーズと初期𝑙音楽特徴）を表す。5.3節で自己回帰生成のための異なるモーションシードを実験している。Transflowerは、モーションと音楽のコンテキストをエンコードするトランスフォーマーエンコーダーと、コンテキストが与えられたときに将来のポーズの確率分布を予測する正規化フローヘッドの2つのコンポーネントから構成されている。ᑝ(𝑥𝑖 , ..., ｜𝑥𝑖-ᑘ𝑥-1;ᑚ𝑖𝑘Ǚ𝑚 )=ᑝ(𝑥𝑖 ｜h) は潜在変数hを条件に正規化フローで表現される。このベクトルは変換エンコーダーh = 𝑓 (𝑥𝑖-ᑘ𝑥-1;𝑚𝑖-𝑘) を通じて文脈を符号化する。エンコーダには、Liら[2021a]が提案したデザインを使用し、コンテキストの動き部分を符号化する2つの変換器hᵅ = 𝑓𝑖-𝑥 , .... , 𝑥𝑖−1) ∈ R 𝑘𝑥×𝑑𝑚 and the music part h𝑥 = 𝑓𝑚 (𝑚𝑖−𝑘𝑚 , ...,𝑚𝑖+𝑙𝑚 ) ∈ R (𝑙𝑚+𝑘𝑚)×𝑑𝑚 separately, where the output dimension 𝑑𝑚 is the same for both pose and music encoders. これら2つの変換器の出力は、クロスモーダル変換器h〜 = 𝑓 (h_1D450↩, hl_1D465↩ ) ∈ R (𝑙+𝑚)×𝑑に連結される。潜在 h∈R 𝐾×𝑑↪Ll_210E はこの出力 h〜の前置に相当する（↪Lu_1D43E の選び方については後述する）。PyTorchの標準的な変換器エンコーダの実装を使用し、時間を超えた翻訳不変性を得るために全ての変換器でT5スタイルの相対位置埋め込み[Raffel et al.2019]を使用する[Wennberg and Henter 2021]。Liら[2021a]はクロスモーダル変換器の出力をモデルの決定論的予測として用いるが、我々はエンコーダの最初のᵃ出力を正規化フロー出力ヘッドを条件付ける潜在ベクトルhとして解釈する。我々は、1x1可逆畳み込みとアフィン結合層に基づく正規化フローモデルを使用する[Henter et al.2020; Kingma and Dhariwal 2018]。Hoら[2019]と同様に、我々はアフィン結合層に注意を用いるが、彼らと異なり、畳み込み層を取り除き、純粋な注意のアフィン結合層を用いる。正規化フローの入力は次元 𝑑 の 𝑁 予測ポーズに対応する．アフィン結合層は入力𝑧𝑖∈R ↪Lu_1D441 ×𝑑𝑥にチャンネルごとに分割し、𝑧 𝑖∈R ↪Lu_1D441 ×𝑑 ／2に対して、𝑧 𝑖に応じたパラメータでアフィン変換する、すなわち、𝑧 𝑖に依存する。 e. 𝑧𝑖+1 = A(𝑧 , h) ⊙𝑧 ′ 𝑖 +B(ᵆ 𝑖 , h) 

これらのアフィンパラメータは結合変換器の出力である (A, B) = ᑓ𝑐𝑡 (𝑥˜) ∈ R 𝑁 ×2𝑥 、ここで𝑥 ′𝑖˜ , h) ∈ R𝑁 ×(𝑑↪Ll_210E) . MoGlow [Henter et al. 2020]と同様に、潜在ベクトルhをチャネル次元に沿ってᑓ𝑡の入力に連結し、コンテキストに正規化フローを条件付ける。主なアーキテクチャの違いは、MoGlowが情報を時間的に伝播させるために主にLSTMに依存しているのに対し、提案モデルはTransformerと注意メカニズムを使用していることである。これにより、モデルが長いコンテキストウィンドウの中で特定の特徴を識別し、焦点を当てることが容易になるはずであり、これは振り付けの学習や一貫したダンスの実行、また音楽に敏感になるために重要かもしれないと考えています。Kingma and Dhariwal [2018]は、非常に小さなバッチサイズ（彼らは1のバッチサイズを使用）を使用したときのバッチ正規化の不正確さに動機づけられてActNorm層を使用しました。我々はより大きなバッチサイズを使用するため、バッチ正規化は時々中程度に速い収束をもたらすことがわかったので、特に指定しない限り、我々のネットワークではActNormの代わりにそれを使用する。我々のモデルでは、これらの正規化フローブロックを16個使用している。また、Liら[2021a]と同様に、次のǔポーズを予測する訓練がモデルの性能を向上させることを発見した。そこで、正規化フローの出力をᵁ×𝑑テンソル（𝑑はポーズベクトルの次元、𝑁はシーケンスの次元）としてモデル化することにした。この設定により、結合層Aの変換器はこのシーケンス次元に沿って作用し、1x1畳み込みはシーケンス内の各要素に対して独立に作用する。変換エンコーダの潜在的な「ベクトル」h は、𝐾 × ↪Ll_1D45E のテンソルとして解釈され、↪Ll_1D45E は変換エンコーダの出力次元である。ᵃと𝑁を等しくすることで、チャンネル次元に沿ってhとAへの入力を連結することができる。図2にアーキテクチャの全体図を示す。動きの特徴 Autodesk Motion Builder を用いて、21 関節（根元を含む）の同じスケルトンに、すべてのモーションデータをリターゲットする。ここで、Δ𝑥とΔ𝑧は、根元の地面投影座標フレーム、すなわち、根元の座標フレームに対する位置の変化である。 Δťは横への移動、Δᑧは前方への移動を表す。ņはベース座標系におけるルートの垂直位置で、床からの高さである。↪Ll_1 は根の地面投影フレームに対する根の関節の3次元回転の指数マップ表現であり、Δņは2次元の向き角の変化である。すべての関節について、回転の指数マップ・パラメトリック[Grassia 1998]を使用し、（非根元）関節ごとに3つの特徴、合計67の運動特徴を得ることができます。音声特徴量。音楽を表現するために、スペクトログラムの特徴とビートに関連する特徴を連結することで組み合わせています。- 80次元のメル周波数対数振幅スペクトル（ホップサイズは50ms）。- Librosa4が提供する1次元のスペクトルフラックスオンセット強度エンベロープ特徴。- MadmomツールボックスのRNNDownBeatProcessorモデルの2次元出力活性化5 . - DeepSaber [Labs 2019]のステージ1のビート検出ニューラルネットワークの最終層の2つの主成分から抽出した2次元ビート特徴で，Dance Dance Convolution [Donahue et al. 2017]のビート検出アーキテクチャと同じだが，Beat Saberレベルで学習させたものである。上記のすべての特徴は、メル周波数スペクトル（20Hz）と同じフレームレートを得るための設定で使用されました。MadmomとDeepSaberの両方の特徴を含めたのは、予備調査において、Madmomビート検出器は曲の規則的なビートを検出するためによく機能したが、DeepSaberの特徴は一般的なオンセット検出器としてよりよく機能することがあることを観察したためである。

上記の特徴量に加工した後、学習に用いるデータセット全体の平均が0、標準偏差が1となるように個別に標準化する。学習 TransflowerとMoGlowベースライン[Henter et al. 2020]の両方を4つのV100 Nvidia GPUで、GPUあたり84のバッチサイズで600kステップ、7日間かけて学習させました。学習率は7×10-5で、200k反復後に0.1倍、400k反復後に再度0.1倍減衰させた。AI Choreographerベースライン[Li et al. 2021a]は、バッチサイズ128（TPUコア毎）、1×10-4の学習率で、1×10-5と1×10-6にそれぞれ100kと200k反復した後、単一のTPUv3上で600k反復で学習させたものです。学習中、我々は「教師強制」を使用します。つまり、モデルへの入力は、モデル出力から自己回帰的にではなく、グランドトゥルースデータから来るのです。異なるモデルのアーキテクチャハイパーパラメータは付録Aの表5に示されており、これらのハイパーパラメータがどのように選択されたかも説明されている。合成。Transflower と MoGlow はともに Nvidia V100 GPU 上で 20 Hz 以上で動作し，AI Choreographer の変換器は Nvidia V100 上で 100 Hz で動作します．ファインチューニング PMSDモーションデータセット（我々のデータの中で最も高品質なモーショントラッキングができる部分）を使って、Transflowerのファインチューニングの効果を調査しています。このデータセットに対してのみ、50k反復の追加トレーニングを行いました。より長く学習することで多様性が減少することがわかり、50k回の学習は5章で述べるように多様性と生成されたモーションの品質向上の間の良いトレードオフを生み出すことがわかりました。なお、ベースラインはこのような微調整を行っていないため、この微調整を行ったシステムと直接比較するべきでは無い。モーションの「促し 我々はまた、自己回帰的なモーション生成におけるモーションシードの役割について調査した。我々は、異なるモデルに5つの異なる種（6秒の長さで、我々のデータセットの異なるスタイルを代表するように選択）を播き、種がダンスのスタイルにどのように影響を与えるかを比較した。我々は、種が実際にダンスのスタイルを制御するために使用することができ、言語モデルにおける現在の傾向[Brownら2020; Reynolds and McDonell 2021]と同様の弱い形態の「プロンプティング」として機能することを見出した。より詳細な結果は5.3節で示す。5 実験と評価 我々はTransflowerをAI Choreographer [Li et al. 2021a]の決定論的変換モデルと、注意を使わずこれまでダンス動作合成に適用されていない確率的動作生成モデルMoGlow [Henter et al. 2020]と比較します。実験では、同じデータセット（表1の印のついたコンポーネント）に対して、過去のモーションコンテキスト、過去と未来の音楽コンテキストを同じ量だけ用いて全てのモデルを学習させる。MoGlowとTransflowerを比較すると、コンテキストへの依存性を学習するために注意を用いる場合（Transflower）と、最近のフレーム連結とLSTMを組み合わせて用いる場合（MoGlow）の効果を知ることができます。AI ChoreographerとTransflowerを比較すると、確率的モデル（Transflower）と決定論的モデル（AI Choreographer）を持つことの効果を見出すことができます。5.1 客観的なメトリクス 我々は、モーションがどれだけリアルか、そしてどれだけ音楽とマッチしているかを捉える2つの客観的なメトリクスを調べます。客観的メトリクスの評価には、トレーニングセットにない33曲とモーションシード（トレーニングデータと曲が重複するAIST++を除く）と、トレーニングセットにあるがモーションシードが異なる27曲（AIST++から）のテストセットを使用する。AIST++以外の33曲のうち、18曲はトレーニングセットからランダムに除外したものであり、残りの15曲は手動で追加したものである。各モデルから5つの異なるモーションシードに対してサンプルを生成し、生成された300個のシーケンスのフルセットでメトリクスを評価した。リアルさメトリクス リアルさの評価には、ポーズᑝの分布と、ポーズ、関節速度、関節加速度の情報を捉えた連続する3ポーズの連結の分布（ᑝ𝑖-1, ᑝ𝑖 , ᑝ+1）間のフレシェ距離を用いている。これらの指標をそれぞれフレシェポーズ距離（FPD）、フレシェ動作距離（FMD）と呼ぶ。これらの指標は、平均と分散の正規化を行っていない「生の」ポーズ特徴量に対して計算されたものである。その結果を表2に示します。AI Choreographerは、我々のデータセットの多様性（スタイルとトラッキング方法の両方）を忠実に再現することに苦労していることがわかります。カオス的な動きや平均的なポーズに固まってしまうことが多いことがわかります。MoGlowはより良く、Transflower（ファインチューン、ノンファインチューンの両方）は実際の動きの分布を最も良く捉えています（予想通り、ノンファインチューンの方が若干良い）。

音楽とのマッチング指標 上記のリアリズム指標に加えて、オーディオとモーションのビートから計算されたメトリクスに基づいて、音楽とダンスのリズムの側面を客観的に評価します。これは、比較的容易に追跡可能であるが、良いダンスにおいて重要な唯一の側面ではない。オーディオビートを抽出するために，Madmom ライブラリに含まれる Böck and Schedl [2011]; Krebs et al. [2015] のビートトラッキングアルゴリズムを採用した．ダンスビートを検出するために、我々は音楽のメーターとダンスの動きの周期性の関係についての大規模な研究[Burger et al.2013; Haugen 2014; Misgeld et al.2019; Naveda and Leman 2011; Toiviainen et al.2010] に依存する。具体的には、我々はToiviainenら[2010]の観察を利用し、音声のビート位置が身体の最大下降速度の点（これは股関節で測定するのが簡単です）と密接に一致することを示し、身体の重心が安定したビート情報を提供するというHaugen [2014]; Misgeldら [2019]の観察とも一致する知見を得ました。したがって、Hips-jointのᵆ速度のローカルミニマムとして、運動学的なビート位置を抽出した。様々なシステムのダンスビートから出現するテンポプロセスとオーディオビートとの関係の視覚化を図3に示す。ここでは、1つの完全なダンスについてオーディオおよびビジュアルテンポグラム [Davis and Agrawala 2018] を描いている。視覚的ヒストグラムには、運動学的なビート位置での局所的なHips速度を大きさ情報として使用しました。図3の視覚的なテンポグラムは、音楽が非常に強く規則的なリズム構造を持ち、水平方向のバンドとして見えることを示しています。また、Transflowerのダンス（特に微調整後）は、オーディオビートとの関係が最も強く、オーディオビートの半分（120bpm付近）に顕著な周期性があることが分かる。この周期性は、他のモデルでは、サブ図を通して右に行くにつれてどんどん洗い流され、リズム的に一貫性のない動きを示している。音声と運動のビートの間の整合を定量化するために、生成されたシードと運動の全セットについて、各音楽ビートとその最も近い運動ビートとの間の絶対時間オフセットを計算する。表3は、評価システムのオフセットの平均と標準偏差を、完全な（マッチした）学習データ、およびランダムにペアリングした（ミスマッチした）音楽とモーションの対応値と共に報告する。提案モデルは、ベースラインシステムと比較して、アライメントを改善することが明らかである。

5.2 ユーザスタディ 定性的なユーザスタディは生成モデルを評価する上で重 要である．なぜなら，多くのダウンストリームアプリケーションで は，異なるメトリックの知覚品質が最も適切なメトリックと なる傾向があるからである．そこで、我々は、我々のモデルとベースラインを、動作の自然さ、音楽への適切さ、ダンスの動きの多様性という3つの軸で評価するためのユーザスタディを実施した。ユーザスタディを行うにあたり、トレーニングセットに存在しない17曲を使用した。このうち、10曲はYouTubeから取得したもので、必ずしも我々のデータセットに含まれるジャンルと一致しない可能性があり、またグランドトゥルースダンスが利用できないものである。PMSDCasualは、一般的なTポーズから構成されているため、固定モーションの種を使用します。これは、PMSDCasualのTポーズが、どのモーションが続くかの不確実性が特に高く、決定論的モデルが平均ポーズに回帰する可能性が高いためと思われます。そこで、この問題を解決するために、ShaderMotionの種をAI Choreographerに使用し、それでもなお平均的なポーズに回帰するシーケンスを評価から除外しました。その結果，17曲の楽曲と4つのモデルについて，それぞれ15秒間のスキンのアニメーションクリップを作成し，合計68個の刺激を得ることができた．以下に示すように、3つの知覚実験を別々に行った。すべての実験はオンラインで行われ、参加者はクラウドソーシング・プラットフォーム（Prolific）を通じて募集された。刺激の提示順序は参加者間でランダムにした。各実験では、25人の参加者が各ビデオクリップを5点リカートタイプの尺度で評価した。参加者の年齢は22歳から66歳（中央値34）、男性39％、女性61％であった。- 自然さ 各モデルが生成したダンス映像について、参加者に「ダンスの動きはどの程度自然か？1は非常に不自然、5は非常に自然である。音声は削除し、参加者は動作の自然さのみを判断できるようにしました。- 適切さ 各モデルが生成したダンス映像について、「音楽に合わせてキャラクターがどの程度踊っているか」という質問に対して、1〜5の尺度で評価をしてもらいました。1は全く、5は非常に良い。- ダンス内の多様性。モデルには様々な種類の多様性があります。ダンス内の多様性：モデルには様々な多様性があり、モーションシードや曲、あるいは同じ曲の中の異なるポイントで多様な動きを見せることができる。また、確率的なモデルでは、同じ種、同じ曲であっても、異なる動作を生成することができる。本研究では、ダンス生成モデルが実際にどのように使われるかを代表するシナリオとして、シードを固定した場合の同一曲内でのモーションの多様性に注目することにした。そこで、同じ曲の中から、モデルごとにバラバラのダンスを2つ選び、その映像を並べて提示した。その際、「2つのダンスは互いにどの程度違うか」という質問に対して、生成されたモーション（音声なし）を評価するよう被験者に求めた。結果 微調整を施したTransflowerモデルが自然さ、適切さともに最も高く評価され、次いでTransflowerの標準モデルが評価された。図4は、3回の実験における4つのモデルの平均評価を示しています。一元配置分散分析およびポストホック・テューキー多重比較検定を行い、有意差を確認した。自然さについては、モデル間のすべての差異が有意であった（ᵅ < 0.001）。適切性については、MoGlowとAI Choreographerを除くすべての差異が有意であった（ᵅ < 0.001）。多様性については、微調整を行ったTransflowerは他のモデルより多様性が低く（ᵅ < 0.001）、TransflowerはMoGlowより多様性が高い（ᵅ = 0.02）ことが示されました。しかし、Transflowerとベースラインは、データの高品質な成分で微調整されていないため、微調整されていないモデルのみが直接比較できることを再度強調する。

5.3 モーション・プロンプト 自己回帰モデルには，生成を開始するための最初のコンテクスト・シードが必要である．我々は、モデルに異なるモーションの種を与えて実験し、種が生成されたダンスのスタイルに大きな影響を与えることを観察した。また、Transflowerが生成するモーションシードには、様々なスタイルのモーションシードが含まれており、FMDを測定することで、そのスタイルのFMDを求めることができる。結果は表4に示すとおりで、シードによってFMD分布（ひいてはダンスのスタイル）が変化し、5件中3件で最小のFMDが対角（シードとスタイルの一致）上に見られるという意味で、シードによって表されるスタイルに近くなる傾向があることが分かる。これは、言語モデルで観測される「プロンプト化」の効果の弱いバージョンと思われる[Brown et al.2020; Reynolds and McDonell 2021]。我々は、このプロンプティング効果により、データセットとモデルサイズを大きくすることで、モデルをより強力にし、モーションとダンス生成モデルの制御性をより高めることができると推測している。また、表4では、あるスタイルではFMDが他のスタイルよりもずっと高いことを観察しています。フリースタイル（SM1からのデータ）は、すべてのシードでFMDが非常に高いことから、モデルが捕捉するのが最も困難であるように思われます。これは、このデータセットが非常に多様であり、モーショントラッキングの品質が低いため、予想されることです。このスタイルを割り引くと、最小のFMDは4つのケースのうち3つで対角線上にあることがわかる。6 結論 この研究では、正規化フローに基づく強力な確率的モデリングと、動きと音楽のコンテキストをエンコードするための注意ベースのエンコーダを組み合わせた、ダンス合成のための最初のモデルについて説明しました。我々は、これらの特性が両方とも重要であることを、我々のモデルを以前に提案された2つのモデルと比較することにより、示した。このモデルは、自己回帰正規化フローとLSTMコンテキストエンコーダに基づいており、ダンス生成にはこれまで適用されていない一般的なモーション合成モデルMoGlow [Henter et al.2020]と、クロスモーダル注意エンコーダに基づいている決定論的ダンス生成モデルAI Choregographer [Li et al.2021a]です。第5章では、TransflowerはMoGlowやAI Choreographerよりもポーズや動きのグランドトゥルース分布にマッチしており（表2）、人間被験者による自然さや音楽への適切さにおいても上位にランキングされていることがわかった（5.2節）。図3のキネマティックテンポグラムでも同様の傾向が見られ、モーションが音楽にどれだけマッチしているか、より客観的に見ることができます。さらに、2つのベースラインを比較すると、MoGlow（確率的であるが変換器を持たない）は、決定論的で変換器ベースのAI Choreographerよりも大幅に優れた自然さの評価を達成したことがわかる。これは、今回の評価で良い結果を得るためには、確率的なアプローチが特に重要であることを示す証拠であると考えられます。また，予備実験では，AIST++のデータのみで学習させた場合，全トレーニングセットで学習させた場合よりも，より自然な動きを生成できることがわかりました．AI Choreographerが比較的高い自然度のスコアに到達する傾向があった以前の研究では、モデルは一度に1つのデータソースでしか訓練されていませんでした[Li et al. 2021b,a]。これらを総合すると、我々のデータセットの高い多様性と異質性が決定論的モデルの性能を著しく低下させ、一方、確率論的モデルは異質性に適応する能力が高いことが示唆される。また、AIST++だけで学習させたTransflowerからのランダムサンプリングでも、モーション品質が向上することが示されました。

自然言語の結果[Henighan et al. 2020]とは異なり、データセットサイズの増大とデータの多様性の増大および品質の低下の可能性を交換した場合、おそらく生成されるダンスの多様性を除いて、好ましいスケーリング動作の証拠は見いだせなかった。しかし、この一部は出力生成方法の違いによるものかもしれない。我々の最強の言語モデル[Brown et al. 2020]は、学習した分布の最も確率の高い結果の中からだけサンプリングする（しばしば「温度を下げる」と呼ばれる）ことで利益を得ているが、我々の実験は温度を下げずにフローによって学習した分布から直接サンプリングしているのだ。また、1つのダンスについて、異なる時点で見られる動きの多様性という観点からもモデルを評価した。図4では、TransflowerはMoGlowよりもスコアが高いものの、AI Choreographerとの差は大きくはないことがわかります。しかし、AI Choreographerの自然度のスコアが低いことを考えると、AI Choreographerの動きの多様性の高さは、ダンスとして意味のある多様性とは言えないかもしれません。また、AI Choreographerは、より混沌とした動きをする傾向があり、さらに、固まったポーズに回帰する可能性が高いことがわかりました（後者の刺激は、ユーザ調査には含まれていません）。そのため、Transflowerはベースラインよりも意味のある多様性を実現したと考えられます。また、TransflowerはPMSDのダンスデータのみで、5万回以上の繰り返し学習を行いました。その結果、微調整を行わない場合と比較して、より自然な動きとなり、また、ユーザー調査の参加者からも、より音楽に適した動きと判断されることが分かりました（図4）。しかし、この改善は、ダンスの（ダンス内の）多様性の減少を犠牲にしており、これはおそらく、微調整したモデルのフレシェ分布距離が増加したことを説明するものである（表2）。5.3節では、自己回帰運動の種が生成されるダンスに与える影響について調べた。その結果，種がダンスのスタイルに大きな影響を与え，6 秒モーションの種が取られたスタイルに近いダンスが生成される傾向があることを見出した．この効果を言語モデルにおけるプロンプトの効果になぞらえて「モーション・プロンプト」と呼ぶことにする[Reynolds and McDonell 2021]。我々の結果は、これが自己回帰的なダンスとモーションモデルに対するスタイル制御を実現する新しい方法である可能性を示唆している。より人間に近いダンスにアプローチする、より良い学習ベースのダンス合成の研究を推進するために、重要なピースは大規模なダンスデータセットの利用可能性である。本研究では、モーションキャプチャ技術によって得られた3Dダンス動作の最大データセットを紹介する。これは、学習型アプローチだけでなく、データ駆動型ダンス合成一般に有益である（2.2項参照）。また、VRの利用が拡大していることから、今回のような新しい研究の機会を提供できると考えています。最後に、モデルをより大きなデータセットに拡張するための最も効果的な方法を調査することは、今後の研究の興味深い方向性であると考えます。

7 制限事項 ダンス合成のための学習ベースの手法には、ある種の制限がある。上述したように、大量のデータを必要とし、より構造化されたアプローチに比べ、信頼性と制御性に劣る結果をもたらす可能性があります。しかし，その反面，柔軟性があり（他のタスクに適用するための変更が少ない），十分なデータがある場合には，より自然な結果が得られる傾向がある．我々のモデルにより特化すると、正規化フローは他の生成モデリング手法と比較して、ある種の利点と限界を示す[Bond-Taylor et al.2021]。この手法は厳密な尤度最大化を可能にし、安定した学習、サンプルの大きな多様性、十分なデータ量と十分な表現力を持つモデルに対する良質な結果を導く。しかし、少なくとも画像に関しては、生成的敵対ネットワークや変分オートエンコーダなどの他のアプローチと比較して、パラメータ効率が悪く、学習に時間がかかる[Bond-Taylor et al.2021]。さらに、我々が使用するフルアテンショントランスフォーマーエンコーダは、並列化可能性が低いため、GPTのような純粋なデコーダベースのモデルよりも訓練に時間がかかります[Vaswani et al.2017]。性能を維持しつつ、これらの制限を克服するアーキテクチャを探ることは、今後の研究の重要な分野であると考えます。最後に、評価についてもさらなる取り組みが必要だと考えています。これには、Transflowerのような最先端の学習ベースの手法と、ChoreoMaster [Kang et al. 2021]のような現在のモーショングラフベースの手法を、生成されるダンスの自然さ、適切さ、多様性の観点から比較し、さらにこれらが利用できるデータにどう依存するのかを評価することが含まれます。また、2.1節で述べたような物理制約を含むモデルも、運動の自然さの点で有望であり、今後の研究でTransflowerのような制約の少ないモデルと比較されるべきだと考えています。我々が提案するTransflowerモデルは、出力に対する対数確率を厳密に計算できるため、物理ベースのILアルゴリズムに対するポリシーのパラメトリゼーションに使用できることに注目する。しかし、それ以上に、ダンス合成モデルの信頼性と下流タスクとの関連性の観点から、ダンス合成モデルを評価するための最良のメトリクスは何かを理解するための作業を包含している。特に、私たちはビートアライメントを客観的な評価指標の1つとして取り上げましたが、これは主に、他によく開発された客観的な評価指標がないことに起因しています。上手なダンスはビート合わせのタスクではなく、これまでのダンス合成の研究では、ダンスの質が、人間がダンスと音楽のマッチングをどう判断するかを決める大きな要因であることが分かっています。例えば、Liら[2021a]では、データセットからランダムに選んだ音楽とペアリングしたグランドトゥルースのダンスは、音楽とのマッチング度に関して、どのモデルよりも有意に高いランクになりました。これは、以前に観察された、人はほとんどすべての音楽と一緒に演奏された音楽に合わせて、多様な入れ子のリズムで良いダンスの動きを見つける傾向があるという効果と関連しているかもしれません[Avirgan 2013; Miller et al.2013]。このネストしたリズムの多様性は、表3のミスマッチな動きの比較的強い結果を説明することにもつながるかもしれません。複数レベルのリズムの現象は、人間や動物のコミュニケーションの多くの側面に及んでいます[Pouw et al. 2021]。ミスマッチダンスと同様の効果が音声駆動型ジェスチャー生成の関連分野でも観察されており、最近の国際チャレンジ[Kucherenko et al. 2021]では、音声と無関係のミスマッチなグランドトゥルースモーションクリップが、最高評価の合成システムよりも音声にふさわしいと評価されました。ジェスチャーの場合、Yoonら[2019]で観察されたように、データにスピーチと沈黙（ジェスチャー活動が期待されない期間）の両方が含まれていると、マッチしたモーションとミスマッチしたモーションの間の格差は評価者にとってより明白となります。これをダンスに戻すと、沈黙を挟んだいくつかの曲や、長時間の劇的なポーズを示す音楽に対応するかもしれません。より広義には、これらの知見は、ダンスの動きの質が下流のアプリケーションにとってより重要な要素であることを指摘しているが、ダンスとダンス合成の評価方法と、リズムと文体の適切性から動きの質の側面を分離することに関するさらなる研究の必要性を示唆している。例えば、間奏の多い曲で評価することで、適切なダンスとそうでないダンスの違いをより顕著にすることができるかもしれません。これは、多くのモデルが静止することを困難とする運動生成の結果を反映しています[Henter et al.］ 

8 結論 結論 結論として、我々は、マルチモーダルな文脈を条件とした高次元連続信号の確率的自己回帰モデリング のための新しいモデルを導入し、これを音楽条件付きダンス生成の問題に適用した。我々は、現在最大の3Dダンスモーションデータセットを作成し、それを用いて我々のモデルと2つの代表的なベースラインを比較評価した。その結果、我々のモデルはいくつかのベンチマークにおいて、従来の技術水準よりも向上しており、我々のモデルの2つの主要な特徴である出力の確率的モデリングと入力の注意に基づくエンコーディングは、いずれも音楽に適したリアルで多様なダンスを生成するために必要であることが示された。

図1. 我々は、3Dダンスモーションの最大のデータセットを集約し、それを用いて、新しいモーションの確率的自己回帰モデルであるTransflowerを学習させました。その結果、あらゆる音楽に合わせてダンスを生成できるモデルが得られ、適切性、自然性、多様性の点で高い順位を獲得した。表1. データセットに含まれるダンスデータの出典。PMSDはPMSD dance datasetの構成要素、SMはShaderMotion VR dance datasetの構成要素を指す。この論文でモデルを学習するために使用したコンポーネントを*でマークしています（最近残りのデータを入手し、フルデータセットにモデルをスケールアップする作業を行っています）。SMKonataとSMVibeの共通点は、ダンサーが1人であることです。GrooveNetとJustDanceは、特定のモチーフを繰り返すダンスが多いため、それぞれのスタイルに分類していることに注意してください。GrooveNetは単純なリズムのモチーフが多く、JustDanceはゲーム「JustDance」に登場するモチーフが多く、多様性に富んでいる。 図2. Transflowerのアーキテクチャ。緑色のブロックはニューラルネットワークのモジュールを表し、下から入力を受け、その出力を上のモジュールに与える。アフィン結合層では、分割と連結がチャンネル単位で行われ、「アフィン結合」は、Henterら[2020]、Kingma and Dhariwal [2018]と同様に、結合変換器の出力でシフトとスケールのパラメータが決まる要素ワイズ線形スケーリングである。正規化フローはいくつかのブロックで構成され，それぞれがバッチ正規化，反転可能な1x1畳み込み，アフィン結合層を含んでいる。モーション、オーディオ、およびクロスモーダル変換器は、T5スタイルの相対位置エンコーディングを使用することを除いて、Liら[2021a]と同様に、標準的なフルアテンション変換器エンコーダである。図3. 1つの完全なダンスに対する音楽と運動学的なテンポグラム。左から右へ。音楽、Transflower fine-tuned (TFF)、Transflower (TF)、MoGlow (MG)、AI Choreographer (AIC). 縦軸は周波数に対応し、単位はbpm（Beats per Minute）。 図4. ユーザー調査の結果、平均評価と標準偏差のバー。左から順に、自然さ、適切さ、多様性、4つのモデルについて。Transflower fine-tuned (TFF), Transflower (TF), MoGlow (MG), AI Choreographer (AIC)の4モデルについて。平均値の95%信頼区間は図には示していませんが、3つの実験のすべてのモデルで±0.13以下に相当し、プロットした標準偏差よりもはるかに狭い範囲に収まっています。